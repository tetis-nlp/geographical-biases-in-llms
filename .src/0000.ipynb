{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geographical Biases in Large Language Models (LLMs)\n",
    "\n",
    "This tutorial aims to identify geographical biases propagated by LLMs.\n",
    "\n",
    "1. Spatial disparities in geographical knowledge\n",
    "2. Spatial information coverage in training datasets\n",
    "3. Correlation between geographic distance and semantic distance\n",
    "4. Anomaly between geographical distance and semantic distance\n",
    "\n",
    "**Authors**\n",
    "\n",
    "| Author      | Affiliation            |\n",
    "|-------------|------------------------|\n",
    "| RÃ©my Decoupes    | INRAE / TETIS      |\n",
    "| Mathieu Roche  | CIRAD / TETIS |\n",
    "| Maguelonne Teisseire | INRAE / TETIS            |\n",
    "\n",
    "![TETIS](https://www.umr-tetis.fr/images/logo-header-tetis.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "!pip install -U bitsandbytes\n",
    "!pip install transformers==4.37.2\n",
    "!pip install -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "\n",
    "list_of_models = {\n",
    "    'bert': {\n",
    "        'name': 'bert-base-uncased',\n",
    "        # 'tokenizer': BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "        # 'model': BertModel.from_pretrained('bert-base-uncased'),\n",
    "        'mask': \"[MASK]\",\n",
    "        'type': \"SLM\"\n",
    "    },\n",
    "    'bert-base-multilingual-uncased':{\n",
    "        'name': 'bert-base-multilingual-uncased',\n",
    "        # 'tokenizer': AutoTokenizer.from_pretrained('bert-base-multilingual-uncased'),\n",
    "        # 'model': BertModel.from_pretrained('bert-base-multilingual-uncased'),\n",
    "        'mask': \"[MASK]\",\n",
    "        'type': \"SLM\"\n",
    "    },\n",
    "    'roberta': {\n",
    "        'name': 'roberta-base',\n",
    "        # 'tokenizer': AutoTokenizer.from_pretrained('roberta-base'),\n",
    "        # 'model': RobertaModel.from_pretrained('roberta-base'),\n",
    "        'mask': \"<mask>\",\n",
    "        'type': \"SLM\"\n",
    "    },\n",
    "    'xlm-roberta-base': {\n",
    "        'name': 'xlm-roberta-base',\n",
    "        # 'tokenizer': AutoTokenizer.from_pretrained('xlm-roberta-base'),\n",
    "        # 'model': RobertaModel.from_pretrained('xlm-roberta-base'),\n",
    "        'mask': \"<mask>\",\n",
    "        'type': \"SLM\"\n",
    "    },\n",
    "    'mistral': {\n",
    "        'name': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "        'type': \"LLM_local\"\n",
    "    },\n",
    "    'llama2': {\n",
    "        'name': 'meta-llama/Llama-2-7b-chat-hf',\n",
    "        'type': \"LLM_local\"\n",
    "    },\n",
    "    'chatgpt':{\n",
    "        'name': 'gpt-3.5-turbo-0301',\n",
    "        'type': \"LLM_remote_api\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initiate API Key**\n",
    "\n",
    "- HuggingFace \n",
    "- OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    " \n",
    "HF_API_TOKEN = getpass.getpass(prompt=\"Your huggingFace API Key\")\n",
    "OPENAI_API_KEY = getpass.getpass(prompt=\"Your OpenAI API Key\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spatial Disparities In Geographical Knowledge\n",
    "\n",
    "We will use 2 different types of language models: Small Language Model (SLM) and Large Language Model (LLM):\n",
    "\n",
    "\n",
    "- For SLMs: we will use the HuggingFace library transformers\n",
    "- For LLMs: 2 methods, through API with OpenAI (ChatGPT) or through local inference for Mistral or llama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLMs = {key: value for key, value in list_of_models.items() if 'type' in value and value['type'] == 'SLM'}\n",
    "print(f\"List of SLMs: {[value['name'] for value in SLMs.values()]}\")\n",
    "\n",
    "local_LLMs = {key: value for key, value in list_of_models.items() if 'type' in value and value['type'] == 'LLM_local'}\n",
    "print(f\"List of LLMs local inference: {[value['name'] for value in local_LLMs.values()]}\")\n",
    "\n",
    "api_LLMs = {key: value for key, value in list_of_models.items() if 'type' in value and value['type'] == 'LLM_remote_api'}\n",
    "print(f\"List of LLMs used through a remote API: {[value['name'] for value in api_LLMs.values()]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geo datasets**\n",
    "\n",
    "Retrieve all country information needed through a python library `countryinfo`:\n",
    "\n",
    "- Country Name\n",
    "- Capital\n",
    "- Region / subregion\n",
    "- Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install countryinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from countryinfo import CountryInfo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "import geopandas as gpd\n",
    "\n",
    "country = CountryInfo()\n",
    "\n",
    "countries = []\n",
    "capitals = []\n",
    "regions = []\n",
    "subregions = []\n",
    "coordinates = []\n",
    "\n",
    "for c in list(country.all().keys()):\n",
    "    country_info = CountryInfo(c)\n",
    "    countries.append(c)\n",
    "    try:\n",
    "        regions.append(country_info.region())\n",
    "    except:\n",
    "        regions.append(np.NAN)\n",
    "    try:\n",
    "        subregions.append(country_info.subregion())\n",
    "    except:\n",
    "        subregions.append(np.NAN)\n",
    "    try:\n",
    "        if country_info.geo_json()[\"features\"][0][\"geometry\"][\"type\"] == \"Polygon\":\n",
    "          coordinates.append(Polygon(country_info.geo_json()[\"features\"][0][\"geometry\"][\"coordinates\"][0]))\n",
    "        else: #MultiPolygon : Take the biggest one\n",
    "          polygons = country_info.geo_json()[\"features\"][0][\"geometry\"][\"coordinates\"]\n",
    "          max_polygon = max(polygons, key=lambda x: len(x[0]))\n",
    "          coordinates.append(Polygon(max_polygon[0]))\n",
    "    except:\n",
    "        coordinates.append(np.NAN)\n",
    "    try:\n",
    "        capitals.append(country_info.capital())\n",
    "    except:\n",
    "        capitals.append(np.NAN)\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'Country': countries,\n",
    "    'Capital': capitals,\n",
    "    'Region': regions,\n",
    "    'Subregion': subregions,\n",
    "    'Coordinates': coordinates\n",
    "}\n",
    "\n",
    "df_countries = pd.DataFrame(data)\n",
    "df_countries = gpd.GeoDataFrame(df_countries, geometry='Coordinates')\n",
    "\n",
    "# Display DataFrame\n",
    "df_countries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 SLMs\n",
    "\n",
    "#### 1.1.1 Example\n",
    "\n",
    "Let's ask Roberta-base from which country Taipei is the capital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(task=\"fill-mask\", model='roberta-base')\n",
    "masked_sentence = f'Taipei is capital of <mask>'\n",
    "\n",
    "prediction = fill_mask(masked_sentence)\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Predicted token: {prediction[0]['token_str']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Worldwide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-base'\n",
    "\n",
    "fill_mask = pipeline(task=\"fill-mask\", model=model_name)\n",
    "\n",
    "def self_masking(city):\n",
    "    masked_sentence = f'{city} is capital of <mask>.'\n",
    "    # Use the pipeline to predict the masked token\n",
    "    predictions = fill_mask(masked_sentence)\n",
    "    # Get the predicted token\n",
    "    predicted_token = predictions[0]['token_str']\n",
    "    return predicted_token.lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries[\"predicted_country_from_capital\"] = df_countries[\"Capital\"].apply(self_masking).str.lower()\n",
    "df_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries[f\"correct\"] = df_countries['Country'] == df_countries[f\"predicted_country_from_capital\"]\n",
    "accuracy_by_continent = df_countries.groupby('Region')[f\"correct\"].mean() * 100\n",
    "accuracy_by_continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries.plot(\"correct\", cmap=\"RdYlGn\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Local LLMs\n",
    "\n",
    "Let's do the same with local LLMs. But as they are big models, we need to use quantization them.\n",
    "\n",
    "#### 1.2.1 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from torch import bfloat16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit quantization\n",
    "    bnb_4bit_quant_type='nf4',  # Normalized float 4\n",
    "    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n",
    "    bnb_4bit_compute_dtype=bfloat16  # Computation type\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    token=HF_API_TOKEN\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", token=HF_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = \"Tapei\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Name the country corresponding to its capital: Paris. Only give the country.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"France\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Name the country corresponding to its capital: {city}. Only give the country.\"}]\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(encodeds, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "print(f\"Results: '{decoded}'\")\n",
    "print(f'Parse only the country: {decoded[0].split(\"[/INST] \")[-1].replace(\"</s>\", \"\").lstrip()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Worldwide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "def self_masking(city):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Name the country corresponding to its capital: Paris. Only give the country.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"France\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Name the country corresponding to its capital: {city}. Only give the country.\"}]\n",
    "    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "    # prompt = f\"Name the country corresponding to {city}. Only give the country.\"\n",
    "    model_inputs = encodeds\n",
    "\n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    try:\n",
    "        country = decoded[0].split(\"[/INST] \")[-1].replace(\"</s>\", \"\").lstrip()\n",
    "    except:\n",
    "        try:\n",
    "            country = decoded[0].split(\"[/INST]\")[-1].lstrip()\n",
    "        except:\n",
    "            country = decoded[0]\n",
    "    return country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries[\"predicted_country_from_capital\"] = df_countries[\"Capital\"].apply(self_masking).str.lower()\n",
    "df_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries[f\"correct\"] = df_countries['Country'] == df_countries[f\"predicted_country_from_capital\"]\n",
    "accuracy_by_continent = df_countries.groupby('Region')[f\"correct\"].mean() * 100\n",
    "accuracy_by_continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries.plot(\"correct\", cmap=\"RdYlGn\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Remote LLMs\n",
    "\n",
    "Using OpenAI / ChatGPT\n",
    "\n",
    "#### 1.3.1 Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "city = \"Tapei\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Name the country corresponding to its capital: Paris. Only give the country.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"France\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Name the country corresponding to its capital: {city}. Only give the country.\"}]\n",
    "\n",
    "\n",
    "reponse = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0301\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(f'Results (without parsing): {reponse[\"choices\"][0][\"message\"][\"content\"]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Worldwide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_masking(city):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Name the country corresponding to its capital: Paris. Only give the country.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"France\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Name the country corresponding to its capital: {city}. Only give the country.\"}]\n",
    "    try:\n",
    "        reponse = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0301\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return reponse['choices'][0]['message']['content'].replace(\".\",\"\")\n",
    "    except:\n",
    "        return \"time_out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries[\"predicted_country_from_capital\"] = df_countries[\"Capital\"].apply(self_masking).str.lower()\n",
    "df_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries[f\"correct\"] = df_countries['Country'] == df_countries[f\"predicted_country_from_capital\"]\n",
    "accuracy_by_continent = df_countries.groupby('Region')[f\"correct\"].mean() * 100\n",
    "accuracy_by_continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries.plot(\"correct\", cmap=\"RdYlGn\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 *Going Further*\n",
    "\n",
    "### 1.4.1 Using other LLMs\n",
    "\n",
    "1. Use a new LLMs like meta/Llama-3, Microsoft/Phi-3 or Alibaba/Qwen1.5\n",
    "2. Use other remote API like Cohere or Groq\n",
    "\n",
    "### 1.4.2 Work on prompt engineer\n",
    "\n",
    "Optimize the prompts to reduce the parsing issues"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
