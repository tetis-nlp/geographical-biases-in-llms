{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geographical Biases in Large Language Models (LLMs)\n",
    "\n",
    "This tutorial aims to identify geographical biases propagated by LLMs.\n",
    "\n",
    "1. Spatial disparities in geographical knowledge\n",
    "2. Spatial information coverage in training datasets\n",
    "3. Correlation between geographic distance and semantic distance\n",
    "4. Anomaly between geographical distance and semantic distance\n",
    "\n",
    "**Authors**\n",
    "\n",
    "| Author      | Affiliation            |\n",
    "|-------------|------------------------|\n",
    "| RÃ©my Decoupes    | INRAE / TETIS      |\n",
    "| Mathieu Roche  | CIRAD / TETIS |\n",
    "| Maguelonne Teisseire | INRAE / TETIS            |\n",
    "\n",
    "![TETIS](https://www.umr-tetis.fr/images/logo-header-tetis.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "!pip install -U bitsandbytes\n",
    "!pip install transformers==4.37.2\n",
    "!pip install -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "\n",
    "list_of_models = {\n",
    "    'bert': {\n",
    "        'name': 'bert-base-uncased',\n",
    "        'tokenizer': BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "        'model': BertModel.from_pretrained('bert-base-uncased'),\n",
    "        'mask': \"[MASK]\",\n",
    "        'type': \"SLM\"\n",
    "    },\n",
    "    'bert-base-multilingual-uncased':{\n",
    "        'name': 'bert-base-multilingual-uncased',\n",
    "        'tokenizer': AutoTokenizer.from_pretrained('bert-base-multilingual-uncased'),\n",
    "        'model': BertModel.from_pretrained('bert-base-multilingual-uncased'),\n",
    "        'mask': \"[MASK]\",\n",
    "        'type': \"SLM\"\n",
    "    },\n",
    "    'roberta': {\n",
    "        'name': 'roberta-base',\n",
    "        'tokenizer': AutoTokenizer.from_pretrained('roberta-base'),\n",
    "        'model': RobertaModel.from_pretrained('roberta-base'),\n",
    "        'mask': \"<mask>\",\n",
    "        'type': \"SLM\"\n",
    "    },\n",
    "    'xlm-roberta-base': {\n",
    "        'name': 'xlm-roberta-base',\n",
    "        'tokenizer': AutoTokenizer.from_pretrained('xlm-roberta-base'),\n",
    "        'model': RobertaModel.from_pretrained('xlm-roberta-base'),\n",
    "        'mask': \"<mask>\",\n",
    "        'type': \"SLM\"\n",
    "    },\n",
    "    'mistral': {\n",
    "        'name': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "        'type': \"LLM_local\"\n",
    "    },\n",
    "    'llama2': {\n",
    "        'name': 'meta-llama/Llama-2-7b-chat-hf',\n",
    "        'type': \"LLM_local\"\n",
    "    },\n",
    "    'chatgpt':{\n",
    "        'name': 'gpt-3.5-turbo-0301',\n",
    "        'type': \"LLM_remote_api\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initiate API Key**\n",
    "\n",
    "- HuggingFace \n",
    "- OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    " \n",
    "HF_API_TOKEN = getpass.getpass(prompt=\"Your huggingFace API Key\")\n",
    "OPENAI_API_KEY = getpass.getpass(prompt=\"Your OpenAI API Key\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spatial Disparities In Geographical Knowledge\n",
    "\n",
    "We will use 2 different types of language models: Small Language Model (SLM) and Large Language Model (LLM):\n",
    "\n",
    "\n",
    "- For SLMs: we will use the HuggingFace library transformers\n",
    "- For LLMs: 2 methods, through API with OpenAI (ChatGPT) or through local inference for Mistral or llama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLMs = {key: value for key, value in list_of_models.items() if 'type' in value and value['type'] == 'SLM'}\n",
    "print(f\"List of SLMs: {[value['name'] for value in SLMs.values()]}\")\n",
    "\n",
    "local_LLMs = {key: value for key, value in list_of_models.items() if 'type' in value and value['type'] == 'LLM_local'}\n",
    "print(f\"List of LLMs local inference: {[value['name'] for value in local_LLMs.values()]}\")\n",
    "\n",
    "api_LLMs = {key: value for key, value in list_of_models.items() if 'type' in value and value['type'] == 'LLM_remote_api'}\n",
    "print(f\"List of LLMs local inference: {[value['name'] for value in api_LLMs.values()]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geo datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install countryinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from countryinfo import CountryInfo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "country = CountryInfo()\n",
    "\n",
    "countries = []\n",
    "capitals = []\n",
    "regions = []\n",
    "subregions = []\n",
    "coordinates = []\n",
    "\n",
    "for c in list(country.all().keys()):\n",
    "    country_info = CountryInfo(c)\n",
    "    countries.append(c)\n",
    "    try:\n",
    "        regions.append(country_info.region())\n",
    "    except:\n",
    "        regions.append(np.NAN)\n",
    "    try:\n",
    "        subregions.append(country_info.subregion())\n",
    "    except:\n",
    "        subregions.append(np.NAN)\n",
    "    try:\n",
    "        coordinates.append(country_info.geo_json()[\"features\"][0][\"geometry\"][\"coordinates\"])\n",
    "    except:\n",
    "        coordinates.append(np.NAN)\n",
    "    try:\n",
    "        capitals.append(country_info.capital())\n",
    "    except:\n",
    "        capitals.append(np.NAN)\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'Country': countries,\n",
    "    'Capital': capitals,\n",
    "    'Region': regions,\n",
    "    'Subregion': subregions,\n",
    "    'Coordinates': coordinates\n",
    "}\n",
    "\n",
    "df_countries = pd.DataFrame(data)\n",
    "\n",
    "# Display DataFrame\n",
    "df_countries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 SLMs\n",
    "\n",
    "#### 1.1.1 Example\n",
    "\n",
    "Let's ask Roberta-base from which country Taipei is the capital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(task=\"fill-mask\", model='roberta-base')\n",
    "masked_sentence = f'Taipei is capital of <mask>'\n",
    "\n",
    "prediction = fill_mask(masked_sentence)\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Predicted token: {prediction[0]['token_str']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1.2 World Wild"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Local LLMs\n",
    "\n",
    "Let's do the same with local LLMs. But as they are big models, we need to use quantization them.\n",
    "\n",
    "#### 1.2.1 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from torch import bfloat16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit quantization\n",
    "    bnb_4bit_quant_type='nf4',  # Normalized float 4\n",
    "    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n",
    "    bnb_4bit_compute_dtype=bfloat16  # Computation type\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    token=HF_API_TOKEN\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", token=HF_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = \"Tapei\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Name the country corresponding to its capital: Paris. Only give the country.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"France\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Name the country corresponding to its capital: {city}. Only give the country.\"}]\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(encodeds, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "print(f\"Results: '{decoded}'\")\n",
    "print(f'Parse only the country: {decoded[0].split(\"[/INST] \")[-1].replace(\"</s>\", \"\").lstrip()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Remote LLMs\n",
    "\n",
    "Using OpenAI / ChatGPT\n",
    "\n",
    "#### 1.3.1 Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "city = \"Tapei\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Name the country corresponding to its capital: Paris. Only give the country.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"France\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Name the country corresponding to its capital: {city}. Only give the country.\"}]\n",
    "\n",
    "\n",
    "reponse = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0301\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(f\"Results (without parsing): {reponse[\"choices\"][0][\"message\"][\"content\"]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
