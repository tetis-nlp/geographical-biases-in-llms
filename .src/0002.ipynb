{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Correlation between geographic distance and semantic distance\n",
    "\n",
    "**Authors**\n",
    "\n",
    "| Author      | Affiliation            |\n",
    "|-------------|------------------------|\n",
    "| RÃ©my Decoupes    | INRAE / TETIS      |\n",
    "| Mathieu Roche  | CIRAD / TETIS |\n",
    "| Maguelonne Teisseire | INRAE / TETIS            |\n",
    "\n",
    "![TETIS](https://www.umr-tetis.fr/images/logo-header-tetis.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.37.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initiate API Key**\n",
    "\n",
    "- HuggingFace \n",
    "- OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    " \n",
    "HF_API_TOKEN = getpass.getpass(prompt=\"Your huggingFace API Key\")\n",
    "OPENAI_API_KEY = getpass.getpass(prompt=\"Your OpenAI API Key\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geo Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install countryinfo\n",
    "!pip install shapely\n",
    "!pip install geopandas\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install geopy\n",
    "!pip install plotly-express\n",
    "!pip install --upgrade nbformat\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from countryinfo import CountryInfo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "import geopandas as gpd\n",
    "\n",
    "country = CountryInfo()\n",
    "\n",
    "countries = []\n",
    "capitals = []\n",
    "regions = []\n",
    "subregions = []\n",
    "coordinates = []\n",
    "\n",
    "for c in list(country.all().keys()):\n",
    "    country_info = CountryInfo(c)\n",
    "    countries.append(c)\n",
    "    try:\n",
    "        regions.append(country_info.region())\n",
    "    except:\n",
    "        regions.append(np.NAN)\n",
    "    try:\n",
    "        subregions.append(country_info.subregion())\n",
    "    except:\n",
    "        subregions.append(np.NAN)\n",
    "    try:\n",
    "        if country_info.geo_json()[\"features\"][0][\"geometry\"][\"type\"] == \"Polygon\":\n",
    "          coordinates.append(Polygon(country_info.geo_json()[\"features\"][0][\"geometry\"][\"coordinates\"][0]))\n",
    "        else: #MultiPolygon : Take the biggest one\n",
    "          polygons = country_info.geo_json()[\"features\"][0][\"geometry\"][\"coordinates\"]\n",
    "          max_polygon = max(polygons, key=lambda x: len(x[0]))\n",
    "          coordinates.append(Polygon(max_polygon[0]))\n",
    "    except:\n",
    "        coordinates.append(np.NAN)\n",
    "    try:\n",
    "        capitals.append(country_info.capital())\n",
    "    except:\n",
    "        capitals.append(np.NAN)\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'Country': countries,\n",
    "    'Capital': capitals,\n",
    "    'Region': regions,\n",
    "    'Subregion': subregions,\n",
    "    'Coordinates': coordinates\n",
    "}\n",
    "\n",
    "df_countries = pd.DataFrame(data)\n",
    "df_countries = gpd.GeoDataFrame(df_countries, geometry='Coordinates')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**add Captials coordinates**\n",
    "\n",
    "With OpenStreetMap data through Nominatim geocoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"geoBias-llm\")\n",
    "location = geolocator.geocode(\"Taipei\", language='en')\n",
    "\n",
    "print(f\"lat: {location.latitude}, lon: {location.longitude}\")\n",
    "\n",
    "def capital_coord(city):\n",
    "    loc = geolocator.geocode(city, language='en')\n",
    "    try:\n",
    "        point = Point(loc.longitude, loc.latitude)\n",
    "    except:\n",
    "        point = np.nan\n",
    "    return point\n",
    "\n",
    "df_countries[\"capital_coordinates\"] = df_countries[\"Capital\"].apply(capital_coord)\n",
    "\n",
    "# Change the geometry\n",
    "df_countries = gpd.GeoDataFrame(df_countries, geometry=\"capital_coordinates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "ax =  world.plot(color='lightgrey')\n",
    "\n",
    "df_countries.plot(ax=ax, color=\"red\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 SLMs\n",
    "\n",
    "### 3.1.1 Example\n",
    "\n",
    "Let's compute the correlation between Taipei and other cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city1 = \"Taipei\"\n",
    "city2 = \"Seoul\"\n",
    "city3 = \"Hanoi\"\n",
    "city4 = \"Tokyo\"\n",
    "city5 = \"Singapour\"\n",
    "city6 = \"London\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve **word embedding** from city names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(input_text):\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids).last_hidden_state\n",
    "        return last_hidden_states.mean(dim=1)[0] # for words chunked into subtokens (out of model vocabulary) and [CLS] & [SEP]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "emb1 = word_embedding(city1)\n",
    "emb2 = word_embedding(city2)\n",
    "emb3 = word_embedding(city3)\n",
    "emb4 = word_embedding(city4)\n",
    "emb5 = word_embedding(city5)\n",
    "emb6 = word_embedding(city6)\n",
    "\n",
    "print(f\"Embedding length: {emb1.shape} \\n\\t{emb1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute **semantic similarity** between the cities' embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(f\"Similarity between {city1} and {city2}: {cosine_similarity([emb1], [emb2])}\")\n",
    "print(f\"Similarity between {city1} and {city3}: {cosine_similarity([emb1], [emb3])}\")\n",
    "print(f\"Similarity between {city1} and {city4}: {cosine_similarity([emb1], [emb4])}\")\n",
    "print(f\"Similarity between {city1} and {city5}: {cosine_similarity([emb1], [emb5])}\")\n",
    "print(f\"Similarity between {city1} and {city6}: {cosine_similarity([emb1], [emb6])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute **geodistance** between cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "geo_coord_1 = capital_coord(city1)\n",
    "geo_coord_2 = capital_coord(city2)\n",
    "geo_coord_3 = capital_coord(city3)\n",
    "geo_coord_4 = capital_coord(city4)\n",
    "geo_coord_5 = capital_coord(city5)\n",
    "geo_coord_6 = capital_coord(city6)\n",
    "\n",
    "# distance = geodesic((geo_coord_1.xy[1][0], geo_coord_1.xy[0][0]), (geo_coord_2.xy[1][0], geo_coord_2.xy[0][0])).kilometers\n",
    "\n",
    "print(f\"Distance between {city1} and {city2}: {geodesic((geo_coord_1.xy[1][0], geo_coord_1.xy[0][0]), (geo_coord_2.xy[1][0], geo_coord_2.xy[0][0])).kilometers} km\")\n",
    "print(f\"Distance between {city1} and {city3}: {geodesic((geo_coord_1.xy[1][0], geo_coord_1.xy[0][0]), (geo_coord_3.xy[1][0], geo_coord_3.xy[0][0])).kilometers} km\")\n",
    "print(f\"Distance between {city1} and {city4}: {geodesic((geo_coord_1.xy[1][0], geo_coord_1.xy[0][0]), (geo_coord_4.xy[1][0], geo_coord_4.xy[0][0])).kilometers} km\")\n",
    "print(f\"Distance between {city1} and {city5}: {geodesic((geo_coord_1.xy[1][0], geo_coord_1.xy[0][0]), (geo_coord_5.xy[1][0], geo_coord_5.xy[0][0])).kilometers} km\")\n",
    "print(f\"Distance between {city1} and {city6}: {geodesic((geo_coord_1.xy[1][0], geo_coord_1.xy[0][0]), (geo_coord_6.xy[1][0], geo_coord_6.xy[0][0])).kilometers} km\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Worldwide\n",
    "\n",
    "Build 2 matrices between pairs of Capitals:\n",
    "- Semantic distance (1 - cosine similarity)\n",
    "- Geo distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop row for which we could not find geo coordinates\n",
    "df_countries = df_countries[df_countries[\"capital_coordinates\"].notna()]\n",
    "df_countries[\"capital_coordinates\"]\n",
    "\n",
    "\n",
    "def tensor_to_array(embedding):\n",
    "    try:\n",
    "        return embedding.numpy()\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df_countries[\"capital_embedding_tensor\"] = df_countries[\"Capital\"].apply(word_embedding)\n",
    "df_countries[\"capital_embedding\"] = df_countries[\"capital_embedding_tensor\"].apply(tensor_to_array)\n",
    "df_countries = df_countries.dropna(subset=[\"capital_embedding\"])\n",
    "\n",
    "embedding_array = np.stack(df_countries[\"capital_embedding\"].values)\n",
    "semantic_distance_matrix = 1 - cosine_similarity(embedding_array, embedding_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_geo_distance(df):\n",
    "    coordinates = df[\"capital_coordinates\"].tolist()\n",
    "    num_city = len(coordinates)\n",
    "\n",
    "    # Create an empty distance matrix\n",
    "    distance_matrix = np.zeros((num_city, num_city))\n",
    "\n",
    "    # Calculate distances and populate the distance matrix\n",
    "    for i in range(num_city):\n",
    "        for j in range(i + 1, num_city):\n",
    "            coord1 = (coordinates[i].xy[1][0], coordinates[i].xy[0][0])\n",
    "            coord2 = (coordinates[j].xy[1][0], coordinates[j].xy[0][0])\n",
    "            distance = geodesic(coord1, coord2).kilometers\n",
    "            distance_matrix[i, j] = distance\n",
    "            distance_matrix[j, i] = distance  # Since the distance matrix is symmetric\n",
    "    return distance_matrix\n",
    "\n",
    "geo_distance_matrix = compute_geo_distance(df_countries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import plotly.express as px\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def label_for_plotting(df):\n",
    "    capital = df[\"Capital\"].tolist()\n",
    "    num_capital = len(capital)\n",
    "\n",
    "    # Create an empty distance matrix\n",
    "    label_matrix = np.chararray((num_capital, num_capital), itemsize=30, unicode=True)\n",
    "\n",
    "    # Calculate distances and populate the distance matrix\n",
    "    for i in range(num_capital):\n",
    "        for j in range(i + 1, num_capital):\n",
    "            coord1 = unidecode(capital[i])\n",
    "            coord2 = unidecode(capital[j])\n",
    "            try:\n",
    "                label_matrix[i, j] = f\"{coord1} - {coord2}\"\n",
    "                label_matrix[j, i] = f\"{coord1} - {coord2}\"\n",
    "            except:\n",
    "                print(f\"{coord1} - {coord2}\")\n",
    "    return label_matrix\n",
    "\n",
    "labels_hover = label_for_plotting(df_countries)\n",
    "\n",
    "def plot_scatter(geo_distance_matrix, semantic_distance_matrix, labels_hover, title):\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Geo Distance\": geo_distance_matrix.flatten(),\n",
    "        \"Semantic Distance\": semantic_distance_matrix.flatten(),\n",
    "        \"labels\": labels_hover.flatten()\n",
    "    })\n",
    "    df[\"labels\"].astype('str')\n",
    "\n",
    "    # Perform linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(df[\"Geo Distance\"], df[\"Semantic Distance\"])\n",
    "    line = slope * df[\"Geo Distance\"] + intercept\n",
    "\n",
    "    # Plot with Plotly Express\n",
    "    fig = px.scatter(df, x=\"Geo Distance\", y=\"Semantic Distance\", title=title,\n",
    "                        trendline=\"ols\", trendline_color_override=\"red\",\n",
    "                        labels={\"Geo Distance\": \"Geo Distance\", \"Semantic Distance\": \"Semantic Distance\"},\n",
    "                        hover_name=\"labels\"\n",
    "                        )\n",
    "\n",
    "    # Add R-squared value to the layout\n",
    "    fig.update_layout(annotations=[\n",
    "        dict(\n",
    "            x=0.05,\n",
    "            y=0.95,\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            text=f'R2 = {r_value**2:.2f}',\n",
    "            showarrow=False,\n",
    "            font=dict(size=12),\n",
    "            bgcolor=\"rgba(255, 255, 255, 0.6)\"\n",
    "        )\n",
    "    ])\n",
    "    fig.show()\n",
    "\n",
    "plot_scatter(geo_distance_matrix, semantic_distance_matrix, labels_hover, \"World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in df_countries[\"Region\"].unique():\n",
    "    print(region)\n",
    "    df = df_countries[df_countries[\"Region\"] == region]\n",
    "    embedding_array = np.stack(df[\"capital_embedding\"].values)\n",
    "    semantic_distance_matrix = 1 - cosine_similarity(embedding_array, embedding_array)\n",
    "    geo_distance_matrix = compute_geo_distance(df)\n",
    "    labels_hover = label_for_plotting(df)\n",
    "    plot_scatter(geo_distance_matrix, semantic_distance_matrix, labels_hover,region)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Local LLMs\n",
    "\n",
    "### 3.2.1 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", token=HF_API_TOKEN)\n",
    "model = AutoModel.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", token=HF_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(input_text):\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids).last_hidden_state\n",
    "        return last_hidden_states.mean(dim=1)[0] # for words chunked into subtokens (out of model vocabulary) and [CLS] & [SEP]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "emb1 = word_embedding(city1)\n",
    "emb2 = word_embedding(city2)\n",
    "emb3 = word_embedding(city3)\n",
    "emb4 = word_embedding(city4)\n",
    "emb5 = word_embedding(city5)\n",
    "emb6 = word_embedding(city6)\n",
    "\n",
    "print(f\"Embedding length: {emb1.shape} \\n\\t{emb1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Similarity between {city1} and {city2}: {cosine_similarity([emb1], [emb2])}\")\n",
    "print(f\"Similarity between {city1} and {city3}: {cosine_similarity([emb1], [emb3])}\")\n",
    "print(f\"Similarity between {city1} and {city4}: {cosine_similarity([emb1], [emb4])}\")\n",
    "print(f\"Similarity between {city1} and {city5}: {cosine_similarity([emb1], [emb5])}\")\n",
    "print(f\"Similarity between {city1} and {city6}: {cosine_similarity([emb1], [emb6])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Worldwide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries[\"capital_embedding_tensor\"] = df_countries[\"Capital\"].apply(word_embedding)\n",
    "df_countries[\"capital_embedding\"] = df_countries[\"capital_embedding_tensor\"].apply(tensor_to_array)\n",
    "df_countries = df_countries.dropna(subset=[\"capital_embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in df_countries[\"Region\"].unique():\n",
    "    print(region)\n",
    "    df = df_countries[df_countries[\"Region\"] == region]\n",
    "    embedding_array = np.stack(df[\"capital_embedding\"].values)\n",
    "    semantic_distance_matrix = 1 - cosine_similarity(embedding_array, embedding_array)\n",
    "    geo_distance_matrix = compute_geo_distance(df)\n",
    "    labels_hover = label_for_plotting(df)\n",
    "    plot_scatter(geo_distance_matrix, semantic_distance_matrix, labels_hover,region)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3  Remote LLMs\n",
    "\n",
    "### 3.3.1 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install openai==0.28\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "tok = 'cl100k_base',\n",
    "model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(input_text):\n",
    "    return np.array(model.embed_documents([input_text])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = word_embedding(city1)\n",
    "emb2 = word_embedding(city2)\n",
    "emb3 = word_embedding(city3)\n",
    "emb4 = word_embedding(city4)\n",
    "emb5 = word_embedding(city5)\n",
    "emb6 = word_embedding(city6)\n",
    "\n",
    "print(f\"Embedding length: {emb1.shape} \\n\\t{emb1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Similarity between {city1} and {city2}: {cosine_similarity([emb1], [emb2])}\")\n",
    "print(f\"Similarity between {city1} and {city3}: {cosine_similarity([emb1], [emb3])}\")\n",
    "print(f\"Similarity between {city1} and {city4}: {cosine_similarity([emb1], [emb4])}\")\n",
    "print(f\"Similarity between {city1} and {city5}: {cosine_similarity([emb1], [emb5])}\")\n",
    "print(f\"Similarity between {city1} and {city6}: {cosine_similarity([emb1], [emb6])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Worldwide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in df_countries[\"Region\"].unique():\n",
    "    print(region)\n",
    "    df = df_countries[df_countries[\"Region\"] == region]\n",
    "    embedding_array = np.stack(df[\"capital_embedding\"].values)\n",
    "    semantic_distance_matrix = 1 - cosine_similarity(embedding_array, embedding_array)\n",
    "    geo_distance_matrix = compute_geo_distance(df)\n",
    "    labels_hover = label_for_plotting(df)\n",
    "    plot_scatter(geo_distance_matrix, semantic_distance_matrix, labels_hover,region)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 *Going Further*: \n",
    "\n",
    "### 3.4.1 Using other LLMs\n",
    "\n",
    "### 3.4.2 Build clusters of countries that are semantically close\n",
    "\n",
    "Use K-Means (n=10 clusters) or Hierarchichal Clustering or DBSCAN to cluster countries \n",
    "\n",
    "A low correlation between geographical distance and semantic distance between location embeddings suggests that the semantic distance (captured by the embedding space) is not strongly related to the geographical distance between locations. This could mean that the semantic relationships are more influenced by cultural, historical, or sociological factors rather than geographical distance.\n",
    "\n",
    "Clustering of countries may highlight cultural or historical relationships between countries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
