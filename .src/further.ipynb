{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going further !\n",
    "\n",
    "**Authors**\n",
    "\n",
    "| Author      | Affiliation            |\n",
    "|-------------|------------------------|\n",
    "| Rémy Decoupes    | INRAE / TETIS      |\n",
    "| Mathieu Roche  | CIRAD / TETIS |\n",
    "| Maguelonne Teisseire | INRAE / TETIS            |\n",
    "\n",
    "![TETIS](https://www.umr-tetis.fr/images/logo-header-tetis.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator 1:\n",
    "\n",
    "### 1.1 Work on prompt\n",
    "\n",
    "Optimize the prompts to reduce the parsing issues\n",
    "\n",
    "### 1.2 Use other basic geographical questions\n",
    "\n",
    "- Predict the capital given its country\n",
    "- What are the 3 mosts populated cities per country\n",
    "- ...\n",
    "\n",
    "## Indicator 2:\n",
    "\n",
    "### 2.1 How to explain the very good geographic knowledge of LLMs when, upon questioning their vocabulary, they have few location?\n",
    "\n",
    "**Hypothesis**: LLMs encountered many locations during their training, however, they are drowned out by the quantity of other words. As a result, the subtokens that make up the locations have a good geographical representation when merged.\n",
    "\n",
    "To validate this hypothesis, we could evaluate the proportion of subtokens from LLM and SLM tokenizers.\n",
    "\n",
    "## Indicator 3:\n",
    "\n",
    "### 3.1 Build clusters of countries that are semantically close\n",
    "\n",
    "Use K-Means (n=10 clusters) or Hierarchichal Clustering or DBSCAN to cluster countries \n",
    "\n",
    "A low correlation between geographical distance and semantic distance between location embeddings suggests that the semantic distance (captured by the embedding space) is not strongly related to the geographical distance between locations. This could mean that the semantic relationships are more influenced by cultural, historical, or sociological factors rather than geographical distance.\n",
    "\n",
    "Clustering of countries may highlight cultural or historical relationships between countries.\n",
    "\n",
    "## Indicator 4:\n",
    "\n",
    "### 4.1 Data visualization\n",
    "\n",
    "Can we work on other data visualizations to highlight which countries are at the center of the semantic space and which ones are on the periphery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geo Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install countryinfo\n",
    "!pip install shapely\n",
    "!pip install geopandas\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install geopy\n",
    "!pip install plotly-express\n",
    "!pip install --upgrade nbformat\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from countryinfo import CountryInfo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "import geopandas as gpd\n",
    "\n",
    "country = CountryInfo()\n",
    "\n",
    "countries = []\n",
    "capitals = []\n",
    "regions = []\n",
    "subregions = []\n",
    "coordinates = []\n",
    "\n",
    "for c in list(country.all().keys()):\n",
    "    country_info = CountryInfo(c)\n",
    "    countries.append(c)\n",
    "    try:\n",
    "        regions.append(country_info.region())\n",
    "    except:\n",
    "        regions.append(np.NAN)\n",
    "    try:\n",
    "        subregions.append(country_info.subregion())\n",
    "    except:\n",
    "        subregions.append(np.NAN)\n",
    "    try:\n",
    "        if country_info.geo_json()[\"features\"][0][\"geometry\"][\"type\"] == \"Polygon\":\n",
    "          coordinates.append(Polygon(country_info.geo_json()[\"features\"][0][\"geometry\"][\"coordinates\"][0]))\n",
    "        else: #MultiPolygon : Take the biggest one\n",
    "          polygons = country_info.geo_json()[\"features\"][0][\"geometry\"][\"coordinates\"]\n",
    "          max_polygon = max(polygons, key=lambda x: len(x[0]))\n",
    "          coordinates.append(Polygon(max_polygon[0]))\n",
    "    except:\n",
    "        coordinates.append(np.NAN)\n",
    "    try:\n",
    "        capitals.append(country_info.capital())\n",
    "    except:\n",
    "        capitals.append(np.NAN)\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'Country': countries,\n",
    "    'Capital': capitals,\n",
    "    'Region': regions,\n",
    "    'Subregion': subregions,\n",
    "    'Coordinates': coordinates\n",
    "}\n",
    "\n",
    "df_countries = pd.DataFrame(data)\n",
    "df_countries = gpd.GeoDataFrame(df_countries, geometry='Coordinates')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**add Captials coordinates**\n",
    "\n",
    "With OpenStreetMap data through Nominatim geocoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"geoBias-llm\")\n",
    "location = geolocator.geocode(\"Taipei\", language='en')\n",
    "\n",
    "print(f\"lat: {location.latitude}, lon: {location.longitude}\")\n",
    "\n",
    "def capital_coord(city):\n",
    "    loc = geolocator.geocode(city, language='en')\n",
    "    try:\n",
    "        point = Point(loc.longitude, loc.latitude)\n",
    "    except:\n",
    "        point = np.nan\n",
    "    return point\n",
    "\n",
    "df_countries[\"capital_coordinates\"] = df_countries[\"Capital\"].apply(capital_coord)\n",
    "\n",
    "# Change the geometry\n",
    "df_countries = gpd.GeoDataFrame(df_countries, geometry=\"capital_coordinates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "ax =  world.plot(color='lightgrey')\n",
    "\n",
    "df_countries.plot(ax=ax, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_to_tuple(point):\n",
    "    if point is None:\n",
    "        return (None, None)\n",
    "    else:\n",
    "        return (point.y, point.x)\n",
    "\n",
    "# Create dataset and data loader\n",
    "df_countries = df_countries.dropna(subset=[\"capital_coordinates\"])\n",
    "cities = df_countries[\"Capital\"].to_list()  # list of city names\n",
    "gps_coords = df_countries[\"capital_coordinates\"].apply(point_to_tuple).to_list()  # list of GPS coordinates (lat, lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "# Define the MLP model\n",
    "class CityEmbeddingMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(CityEmbeddingMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the dataset class\n",
    "class CityGpsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, cities, gps_coords):\n",
    "        self.cities = cities\n",
    "        self.gps_coords = gps_coords\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cities)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        city = self.cities[idx]\n",
    "        gps_coord = self.gps_coords[idx]\n",
    "\n",
    "        # Encode city name using RoBERTa\n",
    "        inputs = tokenizer.encode_plus(city,\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=50,\n",
    "                                        padding='max_length',\n",
    "                                        truncation=True,\n",
    "                                        return_attention_mask=True,\n",
    "                                        return_tensors='pt')\n",
    "        city_embedding = roberta_model(inputs['input_ids'], attention_mask=inputs['attention_mask'])[0]\n",
    "    \n",
    "\n",
    "        # Create target GPS coordinates\n",
    "        gps_coord_tensor = torch.tensor(gps_coord, dtype=torch.float)\n",
    "\n",
    "        return city_embedding, gps_coord_tensor\n",
    "\n",
    "dataset = CityGpsDataset(cities, gps_coords)\n",
    "batch_size = 8\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# Initialize MLP model, loss function, and optimizer\n",
    "mlp_model = CityEmbeddingMLP(input_dim=768, hidden_dim=128, output_dim=2)  # 768 is the RoBERTa embedding dimension\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):  # train for 10 epochs\n",
    "    for batch in tqdm(data_loader):\n",
    "        city_embeddings, gps_coords = batch\n",
    "        # city_embeddings = city_embeddings\n",
    "        # gps_coords = gps_coords\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = mlp_model(city_embeddings)\n",
    "        # outputs.shape : [32,1,50,2]\n",
    "        outputs = torch.mean(outputs, dim=2) # Average all 50 tokens\n",
    "        outputs = torch.squeeze(outputs, dim=1) #only on 1 sequence (not 2!)\n",
    "        loss = criterion(outputs, gps_coords)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(data_loader):\n",
    "    try:\n",
    "        print(f\"{i}: {len(d)}\")\n",
    "    except:\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CityGpsDataset(cities, gps_coords)\n",
    "print(len(cities))\n",
    "print(len(gps_coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
